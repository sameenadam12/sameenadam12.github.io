{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from transfermarket.market import Market\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use transferMarkt API to get Premier League Teams ID's to use in URL\n",
    "tm = Market()\n",
    "comp = tm.get_competitions()\n",
    "clubs = tm.get_teams('GB1')\n",
    "promoted_clubs = tm.get_teams('GB2')\n",
    "team_ids = []\n",
    "player_ids = {}\n",
    "player_list = []\n",
    "relegated_ids = [1132, 1031, 350]\n",
    "promoted_ids = [677, 1003, 180]\n",
    "for i in range(0, len(clubs)):\n",
    "    if clubs[i].id not in relegated_ids:\n",
    "        team_ids.append(clubs[i].id)\n",
    "for i in range(0, len(promoted_clubs)):\n",
    "    if promoted_clubs[i].id in promoted_ids:\n",
    "        team_ids.append(promoted_clubs[i].id)\n",
    "for i in range(0, len(team_ids)):\n",
    "    player_list.append(tm.get_players(team_ids[i]))\n",
    "    for j in range(0, len(player_list[i])):\n",
    "        player_ids[player_list[i][j].name] = player_list[i][j].id\n",
    "# save all player id and player names into a text file\n",
    "with open(\"id_data.json\", \"w\", encoding= 'utf-8') as f:\n",
    "  json.dump(player_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the Age for all players based on Wage_Data.xlsx based on previous json output\n",
    "diff_names=[]\n",
    "\n",
    "#define headers for web scraping\n",
    "headers = {\n",
    "    \"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n",
    "}\n",
    "with open(\"id_data1.json\", \"r\", encoding = 'utf-8') as f:\n",
    "  player_ids = json.load(f)\n",
    "player_wages_df = pd.read_excel(\"Wage Data.xlsx\")\n",
    "\n",
    "#Build the URL and request data\n",
    "url = \"https://r.jina.ai//www.transfermarkt.co.uk/fulham-fc/kader/verein/180/saison_id/2023/plus/1\"\n",
    "response = requests.get(url=url, headers=headers)\n",
    "webpage_text = response.text\n",
    "for i in range (0, len(player_wages_df['Player'])):\n",
    "    player_name = str(player_wages_df['Player'][i])\n",
    "    if player_name in webpage_text:\n",
    "      age = re.findall(f\"{player_name}[\\s\\S]+?\\| .* \\((\\d+)\\)\", str(webpage_text))[0]\n",
    "      player_wages_df['Age'][i] = int(age)\n",
    "    else:\n",
    "         continue\n",
    "    try:\n",
    "            id = player_ids[player_name]\n",
    "    except Exception as ex:\n",
    "            diff_names.append(player_name)\n",
    "            continue\n",
    "#player_wages_df.to_excel(\"Wage Data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Injury History for each player based on Player names from URL\n",
    "\n",
    "#define headers for web scraping\n",
    "headers = {\n",
    "    \"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n",
    "}\n",
    "player_wages_df = pd.read_excel(\"Wage Data.xlsx\")\n",
    "diff_names = []\n",
    "player = []\n",
    "inj_data = []\n",
    "for i in range (0, len(player_wages_df['Player'])):\n",
    "    if(player_wages_df['Team'][i] == 'AFC Bouremouth'):\n",
    "        player_name = str(player_wages_df['Player'][i])\n",
    "        try:\n",
    "            id = player_ids[player_name]\n",
    "        except Exception as ex:\n",
    "            diff_names.append(player_name)\n",
    "            continue\n",
    "        url = f\"https://r.jina.ai//www.transfermarkt.co.uk/{player_name}/verletzungen/spieler/{id}\"\n",
    "        response = requests.get(url=url, headers=headers)\n",
    "        webpage_text = response.text\n",
    "        extract = re.findall(\"\\| (\\d+/\\d+) \\| ([\\w ]+) \\| ([\\w, ]+) \\| ([\\w, ]+) \\| ([\\w, ]+) \\|\", str(webpage_text))\n",
    "        for i in range(0, len(extract)):\n",
    "            inj_dict = {'Name': player_name, 'Season': extract[i][0], 'Injury': extract[i][1], 'From date': extract[i][2], 'Till date': extract[i][3], 'Days missed': extract[i][4]}\n",
    "            inj_data.append(inj_dict)\n",
    "        if(response.status_code != 200):\n",
    "            print(response.status_code)\n",
    "        time.sleep(3)\n",
    "inj_df = pd.DataFrame(inj_data)\n",
    "#inj_df.to_csv(\"inj_data.csv\", mode='a+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISCO Analytics - create the ISCO combined dataset based on wage and injury data\n",
    "\n",
    "player_wages_df = pd.read_excel(\"Wage Data.xlsx\")\n",
    "inj_df = pd.read_csv(\"inj_data.csv\")\n",
    "inj_df['Days missed'] = inj_df['Days missed'].str.replace(' days', '').astype(int)\n",
    "\n",
    "total_days_injured_per_player = inj_df.groupby('Name')['Days missed'].sum().reset_index()\n",
    "total_days_injured_per_player.columns = ['Player', 'Total Days Injured']\n",
    "\n",
    "# Calculate injury frequency for each player (number of injuries)\n",
    "injuries_per_player = inj_df.groupby('Name').size().reset_index()\n",
    "injuries_per_player.columns = ['Player', 'Total Injury Count']\n",
    "\n",
    "# Merge the wage data with the total days injured data\n",
    "merged_df = pd.merge(player_wages_df, total_days_injured_per_player, left_on='Player', right_on='Player', how='left')\n",
    "merged_df = pd.merge(merged_df, injuries_per_player, left_on='Player', right_on='Player', how='left')\n",
    "merged_df['Total Days Injured'].fillna(0, inplace=True)\n",
    "merged_df['Total Injury Count'].fillna(0, inplace=True)\n",
    "\n",
    "# Convert 'Annual Salary' and 'Weekly Wage' to numerical values (remove currency symbols and commas)\n",
    "merged_df['Annual Salary'] = merged_df['Annual Salary'].replace('[\\£,]', '', regex=True).astype(float)\n",
    "merged_df['Weekly Wage'] = merged_df['Weekly Wage'].replace('[\\£,]', '', regex=True).astype(float)\n",
    "\n",
    "# Calculate the average days missed per season for each player\n",
    "merged_df['Days Injured per Season'] = (merged_df['Total Days Injured'] / (merged_df['Age'] - 17)).round(2)\n",
    "merged_df['Days Missed per Injury'] = (merged_df['Total Days Injured'] / merged_df['Total Injury Count']).round(2)\n",
    "merged_df['Days Missed per Injury'].fillna(0, inplace=True)\n",
    "merged_df['Days Injured per Season'].fillna(0, inplace=True)\n",
    "merged_df['Injury Risk Score'] = ((merged_df['Age'] * 0.1) + ((merged_df['Total Injury Count'] / merged_df['Age']) * 0.4) + merged_df['Days Injured per Season'] * 0.25 + merged_df['Days Missed per Injury'] * 0.25)\n",
    "\n",
    "# Separate players with zero injuries and classify them as 'No Risk'\n",
    "no_risk_players = merged_df[merged_df['Total Injury Count'] == 0].copy()\n",
    "no_risk_players['Cluster Label'] = 'No Risk'\n",
    "\n",
    "# Filter out players with zero injuries\n",
    "cluster_df = merged_df[merged_df['Total Injury Count'] > 0].copy()\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "features = ['Injury Risk Score', 'Annual Salary']\n",
    "scaled_features = scaler.fit_transform(cluster_df[features])\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(scaled_features)\n",
    "cluster_df['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Manually assign labels based on the cluster characteristics\n",
    "cluster_labels = {0: 'MediumRisk-HighWage', 1: 'HighRisk-LowWage', 2: 'VeryHighRisk-LowWage', 3: 'LowRisk-LowWage', 4: 'LowRisk-HighWage', 5: 'MediumRisk-LowWage'}\n",
    "cluster_df['Cluster Label'] = cluster_df['Cluster'].map(cluster_labels)\n",
    "\n",
    "# Get the cluster centers and transform back to original scale\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "original_centers = scaler.inverse_transform(cluster_centers)\n",
    "\n",
    "# Get the boundaries for each cluster\n",
    "boundaries = {}\n",
    "for label in cluster_df['Cluster'].unique():\n",
    "    cluster_data = cluster_df[cluster_df['Cluster'] == label]\n",
    "    min_values = cluster_data[features].min()\n",
    "    max_values = cluster_data[features].max()\n",
    "    boundaries[label] = {\n",
    "        'Cluster Label': cluster_labels[label],\n",
    "        'Min Values': min_values,\n",
    "        'Max Values': max_values\n",
    "    }\n",
    "\n",
    "# Combine the 'No Risk' players with the clustered players\n",
    "merged_df = pd.concat([no_risk_players, cluster_df], ignore_index=True)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=merged_df, x='Annual Salary', y='Injury Risk Score', hue='Cluster Label', palette=sns.color_palette(\"Set2\", 7))\n",
    "plt.title('Clusters of Players Based on Injury Proneness')\n",
    "plt.xlabel('Annual Salary')\n",
    "plt.ylabel('Injury Risk Score')\n",
    "plt.legend(title='Cluster Label', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Remove duplicate rows\n",
    "merged_df.drop_duplicates(inplace=True)\n",
    "\n",
    "merged_df.to_csv(\"ISCO.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate percentile rank\n",
    "def calculate_percentile(series):\n",
    "    return series.rank(pct=True) * 100\n",
    "\n",
    "#Calculate percentage change proportional to injury risk score\n",
    "\n",
    "isco_df = pd.read_csv(\"ISCO.csv\")\n",
    "\n",
    "#Max wage cut is 30% and min wage cut is 5%\n",
    "maxcut = 30.0\n",
    "mincut = 5.0\n",
    "max_risk = isco_df['Injury Risk Score'].max()\n",
    "min_risk = isco_df[isco_df['Injury Risk Score'] > mincut]['Injury Risk Score'].min()\n",
    "isco_df['percent_change'] = 0.0\n",
    "isco_df['FitWage'] = isco_df['Weekly Wage']\n",
    "isco_df['InjuredWage'] = isco_df['Weekly Wage']\n",
    "for i in range(0, len(isco_df['Injury Risk Score'])):\n",
    "    if(isco_df['Injury Risk Score'][i]>mincut):\n",
    "        percent_change = (maxcut - ((maxcut - mincut) * ((max_risk - min_risk - isco_df['Injury Risk Score'][i]) / (max_risk - min_risk)))).round(2)\n",
    "        isco_df.at[i, 'percent_change'] = percent_change\n",
    "        if \"LowWage\" in isco_df.at[i, 'Cluster Label']:\n",
    "            isco_df.at[i, 'FitWage'] = isco_df['Weekly Wage'][i] + (isco_df['Weekly Wage'][i] * percent_change / 200).astype(int)\n",
    "        else:\n",
    "            isco_df.at[i, 'FitWage'] = isco_df['Weekly Wage'][i]\n",
    "        isco_df.at[i, 'InjuredWage'] = isco_df['Weekly Wage'][i] - (isco_df['Weekly Wage'][i] * percent_change / 100).astype(int)\n",
    "    else:\n",
    "        isco_df.at[i, 'percent_change'] = 0\n",
    "        isco_df.at[i, 'FitWage'] = isco_df['Weekly Wage'][i]\n",
    "        isco_df.at[i, 'InjuredWage'] = isco_df['Weekly Wage'][i]\n",
    "\n",
    "#Recommendation sentence based on Classification\n",
    "\n",
    "    match isco_df.at[i, 'Cluster Label']:\n",
    "        case \"MediumRisk-HighWage\":\n",
    "            isco_df.at[i, 'Recommendation'] = \"High Potential (Careful Management): Maximise value with strategic management\"\n",
    "        case \"HighRisk-LowWage\":\n",
    "            isco_df.at[i, 'Recommendation'] = \"Depth Option: Consider for backup option, injury risk a major factor.\"\n",
    "        case \"VeryHighRisk-LowWage\":\n",
    "            isco_df.at[i, 'Recommendation'] = \"High-Risk Gamble: Potential upside, but cautious approach needed.\"\n",
    "        case \"LowRisk-LowWage\":\n",
    "            isco_df.at[i, 'Recommendation'] = \"Reliable Depth: Solid contributors, good value.\"\n",
    "        case \"LowRisk-HighWage\":\n",
    "            isco_df.at[i, 'Recommendation'] = \"Core Star: Build your team around them.\"\n",
    "        case \"MediumRisk-LowWage\":\n",
    "            isco_df.at[i, 'Recommendation'] = \"Value Rotation: Good balance, valuable backups.\"\n",
    "        case \"No Risk\":\n",
    "            isco_df.at[i, 'Recommendation'] = \"Dream Player: Minimal risk of injury in the future.\"\n",
    "\n",
    "# Calculate percentile rank for 'Injury Risk Score' within each 'POS' and 'Team'\n",
    "isco_df['Injury Risk Percentile_POS'] = 100 - isco_df.groupby(['POS'])['Injury Risk Score'].transform(calculate_percentile).round(2)\n",
    "isco_df['Injury Risk Percentile_Team'] = 100 - isco_df.groupby(['Team'])['Injury Risk Score'].transform(calculate_percentile).round(2)\n",
    "\n",
    "# Calculate percentile rank for 'Annual Salary' within each 'POS' and 'Team'\n",
    "isco_df['Salary Percentile_POS'] = isco_df.groupby(['POS'])['Annual Salary'].transform(calculate_percentile).round(2)\n",
    "isco_df['Salary Percentile_Team'] = isco_df.groupby(['Team'])['Annual Salary'].transform(calculate_percentile).round(2)\n",
    "\n",
    "#Create final CSV data file\n",
    "isco_df.to_csv(\"ISCO.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
